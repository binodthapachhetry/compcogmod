{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports for rendering to a virtual display (needed in docker container)\n",
    "from pyvirtualdisplay import Display\n",
    "d = Display(visible=0, size=(1400, 900))\n",
    "d.start()\n",
    "\n",
    "import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Imports specifically so we can render outputs in Jupyter.\n",
    "from JSAnimation.IPython_display import display_animation\n",
    "from matplotlib import animation\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The OpenAI Gym is a general purpose tool for developing, evaluating and comparing reinforcement learning algorithms (check out https://gym.openai.com/docs/ and https://github.com/openai/gym). The gym provides a common interface to a wide range of reinforcement learning tasks. Each task is instantiated as a python object called an \"Environment.\" Each environment has an action space and an observation space. By calling the environment's \"step\" function, the user can repeatedly input new actions to the environment and get back rewards and a new observations.\n",
    "\n",
    "The gym doesn't come with any reinforcement learning algorithm implementations, only the environment. Creating a learning agent is up to you! This gives you the flexibility to create an agent with standard python, or a machine learning library such as pyTorch.\n",
    "\n",
    "In this lab, we will train a simple agent to perform the \"cart-pole\" task, in which it tries to balance a pole so that it doesn't fall from vertical for as long as possible. The available actions to the agent are to move the cart left or right. And the observations observed include the position x of the cart, the velocity of x, the angle $\\theta$ of the pole, and the angular velocity of $\\theta$. The agent gets a reward of 1 for each time step where the pole hasn't fallen too far from vertical. Running the rest of the cells in this notebook without modification will produce a video of an agent performing the cart-pole task with a random action policy. Each time the pole moves too far from vertical, the task will restart in a new episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will use a \"tabular\" Q-learning approach, in which we divide the continuous (and thus infinite) observation space into a finite number of discrete states. We will iteratively update the Q-value table based on experience, with the goal of having each Q value represent the expected cumulative reward of taking a given action in a given state, and following the optimal policy thereafter. As a reminder, the update function of Q-learning is\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha (r_{t+1} + \\max_{a}Q(s_{t+1}, a) - Q(s_t, a_t))\n",
    "$$\n",
    "\n",
    "(Since we are limiting the length of the task, we will assume no discounting.) More on Q-learning can be found at: http://incompleteideas.net/book/ebook/node65.html\n",
    "\n",
    "In the homework, you will use a neural network to approximate the Q-value function over the continuous state space, rather than assuming a simple discretization. Discretization works for low-dimensional state spaces such as this one, but run into a \"curse of dimensionality\" for high-dimensional state spaces.\n",
    "\n",
    "Our Q learning agent will take actions based on an $\\epsilon$-greedy policy. This means that with probability $1-\\epsilon$ the agent will take what it thinks to be the optimal action, and with probability $\\epsilon$ it will take a random action instead. This type of policy ensures that the agent adequately explores the state space, and doesn't only learn about the action(s) it initially believes to be best. (see http://incompleteideas.net/book/ebook/node16.html for more information.)\n",
    "\n",
    "Below is incomplete code for a Q-learning agent. Your task will be to complete `select_action` and `update_q_table` methods to create $\\epsilon$-greedy Q learner.\n",
    "\n",
    "(This code is modified from https://medium.com/@tuzzer/cart-pole-balancing-with-q-learning-b54c6068d947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearner:\n",
    "    def __init__(self, env):\n",
    "        # state representation size for x, x', theta, and theta'\n",
    "        self.num_buckets = (3, 3, 6, 3)\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.state_bounds = list(zip(env.observation_space.low, env.observation_space.high))\n",
    "        self.state_bounds[1] = [-0.5, 0.5]\n",
    "        self.state_bounds[3] = [-math.radians(50), math.radians(50)]\n",
    "        self.q_table = np.zeros(self.num_buckets + (self.num_actions,))\n",
    "        self.learning_rate = .5\n",
    "        self.explore_rate = 1.0\n",
    "\n",
    "    def select_action(self, obs):\n",
    "        # store state for later update\n",
    "        state = self.state_to_bucket(obs)\n",
    "        \n",
    "        # REPLACE THIS (takes random action)\n",
    "        action = env.action_space.sample()\n",
    "        # TODO: take random action with probability self.explore_rate,\n",
    "        # and take optimal action otherwise.\n",
    "        # Hint: optimal action can be found with np.argmax(self.q_table[state])\n",
    "        \n",
    "\n",
    "        \n",
    "        #save state for later update\n",
    "        self.last_state = state\n",
    "        return action\n",
    "    \n",
    "    def update_q_table(self, obs, action, reward):\n",
    "        new_state = self.state_to_bucket(obs)\n",
    "        best_q = np.amax(self.q_table[new_state])\n",
    "        old_val = self.q_table[self.last_state + (action,)]\n",
    "        \n",
    "        # REPLACE THIS\n",
    "        self.q_table[self.last_state + (action,)] = old_val\n",
    "        # TODO: use self.learning_rate to perform an update on the current state\n",
    "        \n",
    "        \n",
    "    def state_to_bucket(self, obs):\n",
    "        #this function turns continuous states into discrete 'binned' states,\n",
    "        #letting us perform tabular Q learning\n",
    "        bucket_indice = []\n",
    "        for i in range(len(obs)):\n",
    "            if obs[i] <= self.state_bounds[i][0]:\n",
    "                bucket_index = 0\n",
    "            elif obs[i] >= self.state_bounds[i][1]:\n",
    "                bucket_index = self.num_buckets[i] - 1\n",
    "            else:\n",
    "                # Mapping the state bounds to the bucket array\n",
    "                bound_width = self.state_bounds[i][1] - self.state_bounds[i][0]\n",
    "                offset = (self.num_buckets[i]-1)*self.state_bounds[i][0]/bound_width\n",
    "                scaling = (self.num_buckets[i]-1)/bound_width\n",
    "                bucket_index = int(round(scaling*obs[i] - offset))\n",
    "            bucket_indice.append(bucket_index)\n",
    "        return tuple(bucket_indice)\n",
    "    \n",
    "    def update_learning_explore_rates(self, t):\n",
    "        # gradually shrink explore and learning rate on later epochs\n",
    "        self.explore_rate = max(.01, min(1, 1.0 - math.log10((t+1)/25)))\n",
    "        self.learning_rate =  max(.1, min(0.5, 1.0 - math.log10((t+1)/25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate(agent):\n",
    "    ## Defining the simulation related constants\n",
    "    NUM_EPISODES = 600\n",
    "    STREAK_TO_END = 50\n",
    "    SOLVED_T = 199\n",
    "    num_streaks = 0\n",
    "    time_step_list = []\n",
    "    for episode in range(NUM_EPISODES):\n",
    "        # Reset the environment\n",
    "        obs = env.reset()\n",
    "        agent.update_learning_explore_rates(episode)\n",
    "        for t in range(SOLVED_T+1):\n",
    "            #env.render()\n",
    "            # Select an action\n",
    "            action = agent.select_action(obs)\n",
    "            # Execute the action and observe new state and reward\n",
    "            obs, reward, done, _ = env.step(action)\n",
    "            # Update the Q based on the result\n",
    "            agent.update_q_table(obs, action, reward)\n",
    "            if t == SOLVED_T:\n",
    "                done = True\n",
    "            if done:\n",
    "                time_step_list.append(t)\n",
    "                print(\"Episode %d finished after %f time steps\" % (episode, t))\n",
    "                if t == SOLVED_T:\n",
    "                    num_streaks += 1\n",
    "                else:\n",
    "                    num_streaks = 0\n",
    "                break\n",
    "        # It's considered done when it's solved over 120 times consecutively\n",
    "        if num_streaks > STREAK_TO_END:\n",
    "            break\n",
    "    return time_step_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_video(agent, n_frames):\n",
    "    num_episodes=100\n",
    "    frames = []\n",
    "    for i_episode in range(num_episodes):\n",
    "        obs = env.reset()\n",
    "        for t in range(200):\n",
    "            # Render into buffer. \n",
    "            frames.append(env.render(mode = 'rgb_array'))\n",
    "            if len(frames) > n_frames:\n",
    "                break\n",
    "            action = agent.select_action(obs)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "        if len(frames) > n_frames:\n",
    "            break\n",
    "    return frames\n",
    "def display_frames_as_gif(frames):\n",
    "    \"\"\"\n",
    "    Displays a list of frames as a gif, with controls\n",
    "    \"\"\"\n",
    "    #plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n",
    "    display(display_animation(anim, default_mode='loop'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "agent = QLearner(env)\n",
    "episode_lengths = simulate(agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(episode_lengths)), episode_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_frames_as_gif(make_video(agent, 200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Right now our QLearner just takes a random action and learns... nothing. Modify the `select_action` and `update_q_table` methods to turn our learner into an $\\epsilon$-greed Q learner!\n",
    "\n",
    "Task 2: This agent learns, but not as quickly as we like. Take a look at the agent performing the task. What seem to be the relevant environmental variables? How might we modify the state space to facilitate faster learning?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
